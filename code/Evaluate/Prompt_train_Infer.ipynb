{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ff229-0977-4f57-92b1-4d6972ff785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs2926/miniconda3/envs/soft-py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* loaded configs\n",
      "* OPENAI_API_BASE_URL=http://localhost:11434/v1/\n",
      "* OPENAI_API_MODEL=llama3.1\n",
      "* PUBMED_DATA_TSV=/data/pubmed/metadata_36m.tsv\n",
      "* OUTPUT_DATABASE=./database/software_names\n",
      "* HF_TOKEN=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\n",
      "* PYTHONPATH=/home/rs2926/workspace/Softname_Extraction/code/Gpt_4o_mini/:/home/rs2926/workspace/Softname_Extraction/code/Llama3.1/:\n",
      "* loaded all libraries\n",
      "* loaded custom openai client at http://localhost:11434/v1/\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from ner_metrics_both import classification_report\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# -- 加入llama3.1的infer模块 -- #\n",
    "load_dotenv()\n",
    "LLama31_infer_path = os.getenv('LLama31_infer_path')\n",
    "sys.path.append(LLama31_infer_path)\n",
    "from llama_infer import *\n",
    "\n",
    "# -- 加入gpt-4o的infer模块\n",
    "load_dotenv()\n",
    "gpt_infer_path = os.getenv('GPT_infer_path')\n",
    "OPENAI_APIKEY = os.getenv(\"OPENAI_APIKEY\")\n",
    "sys.path.append(gpt_infer_path)\n",
    "from Inference_pubmed import *\n",
    "\n",
    "\n",
    "# -- 加入gpt-4o的infer代码 -- #\n",
    "\n",
    "\n",
    "\n",
    "# 不做删除，直接占位，之后给成O，保持和ann处理之后的长度一致！\n",
    "\n",
    "def process_special_token(orignial_list):\n",
    "    \"\"\"\n",
    "    处理gpt/llama3.1的输出，保持原有text不变\n",
    "    input: [[],[],[]]\n",
    "    TODO: 如果最里边的[]。两边加空格：-,/。左边加空格：!,+。拆开：（）\n",
    "    output: 直接在原列表上修改\n",
    "    \"\"\"\n",
    "    return_list = deepcopy(orignial_list)\n",
    "    for entity in orignial_list:\n",
    "        # 每一个item是一个列表\n",
    "        dup_entity = deepcopy(entity)\n",
    "        count = 0 # 删除的个数\n",
    "\n",
    "        # each_index 是entity的坐标索引\n",
    "        # dup_index = each_index-count 是dup_entity的坐标索引\n",
    "\n",
    "        for each_index in range(0,len(entity)):\n",
    "            dup_index = each_index - count\n",
    "\n",
    "            if each_index == 0:\n",
    "                continue\n",
    "\n",
    "            # 有[\"a\", \"-\", \"b\"], [\"a\", \"/\", \"b\"]。加入[\"a-b\"]\n",
    "            if (entity[each_index] == \"-\") or (entity[each_index] == \"/\"):\n",
    "                dup_entity[dup_index-1] = dup_entity[dup_index-1]+dup_entity[dup_index]+dup_entity[dup_index+1]\n",
    "                del dup_entity[dup_index]\n",
    "                del dup_entity[dup_index]\n",
    "                count += 2\n",
    "                return_list.append(dup_entity)\n",
    "\n",
    "            \"\"\"\n",
    "            # 有[\"a-b\"]。加入[\"a\",\"-\",\"b\"]\n",
    "            elif (\"-\" in entity[each_index]) or (\"/\" in entity[each_index]):\n",
    "                index_1 = entity[each_index].find(\"-\")\n",
    "                index_2 = entity[each_index].find(\"/\")\n",
    "\n",
    "                index = 0\n",
    "                if index_1 == -1: index = index_2\n",
    "                else: index = index_1\n",
    "\n",
    "                if index < len(dup_entity[dup_index])-1:\n",
    "                    dup_entity[dup_index] = dup_entity[dup_index][0:index]\n",
    "                    dup_entity.insert(dup_index+1, dup_entity[dup_index][index+1:])\n",
    "                    print(index, len(dup_entity[dup_index]))\n",
    "                    dup_entity.insert(dup_index+1, dup_entity[dup_index][index])\n",
    "                    count -= 2 \n",
    "                    return_list.append(dup_entity)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "    return return_list\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "# --- 子列表匹配，返回匹配的首、尾对应的元组列表 --- #\n",
    "def find_sublist(main_list, sub_list):\n",
    "    \"\"\"\n",
    "    对处理后的gpt/llama3.1的输出，把原来的text和每一个entity做匹配\n",
    "    \"\"\"\n",
    "    sub_len = len(sub_list)\n",
    "    index_list = list()\n",
    "    flag_in = False\n",
    "    for i in range(len(main_list) - sub_len + 1):\n",
    "        #if main_list[i:i + sub_len] == sub_list:\n",
    "        if (\"\").join(sub_list) in (\"\").join(main_list[i:i + sub_len]):\n",
    "            # 解决可能有分割不准确的情况，子列表->子字符串匹配\n",
    "            index_list.append((i,i+sub_len-1))\n",
    "            flag_in = True\n",
    "    if flag_in == True: # 找到了\n",
    "        return index_list\n",
    "    else: # 没找到 \n",
    "        return -1  \n",
    "    \n",
    "\n",
    "\n",
    "# ------- llama3.1输出结果生成BIO tag，为了评估 ------- #\n",
    "# input: 原始文本, 抽取结果\n",
    "# output: ['O', 'B-PER', 'O', 'B-ORG', 'B-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'O']\n",
    "def convert_txt_to_bio(text, entities):\n",
    "\n",
    "    \"\"\"\n",
    "    逻辑得修改，是对entity做preprocess，而不是对text做啦。保证列表长度一致\n",
    "    \"\"\"\n",
    "    # 原始文本全部变成小写\n",
    "    text = text.lower()\n",
    "    no_spaces_text = text.split(\" \")\n",
    "\n",
    "    # software name变成[[],[]]的格式，做子列表的匹配\n",
    "    software_names = list(set([item[\"name\"] for item in entities]))\n",
    "    # print(\"*\"*5, software_names)\n",
    "    software_names_list = [item.lower().split(\" \") for item in software_names]\n",
    "    software_names_list = process_special_token(software_names_list)\n",
    "    print(\"#\"*5, software_names_list)\n",
    "\n",
    "    bio_index = list()\n",
    "    for item in software_names_list:\n",
    "        # 子列表的匹配\n",
    "        current_index_list = find_sublist(no_spaces_text, item)\n",
    "        if current_index_list == -1:\n",
    "            print(\"txt to bio\",\"=\"*10, item)\n",
    "        else:\n",
    "            bio_index += current_index_list\n",
    "\n",
    "    bio_list = [\"O\"] * len(no_spaces_text)\n",
    "\n",
    "    # 遍历每个索引对，并设置对应位置为B或者I\n",
    "    for start, end in bio_index:\n",
    "        for i in range(start, end + 1):  # 因为end是包含的，所以用end + 1\n",
    "            if i == start:\n",
    "                bio_list[i] = \"B\"\n",
    "            else:\n",
    "                bio_list[i] = \"I\"\n",
    "    return bio_list\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 调用gpt-4o和llama3.1来推理 -------- # \n",
    "\n",
    "def gpt_4o_infer(paper):\n",
    "\n",
    "    '''\n",
    "    Extract something from the abstract of a paper based on the given prompt template.\n",
    "    '''\n",
    "    print(\"* gpt-4o is inferring\")\n",
    "    system_role = SYSTEM_ROLE\n",
    "    prompt_template = TPL_PROMPT\n",
    "    guidelines = \"\"\"\"\"\"\n",
    "    few_shots = \"\"\"\"\"\"\n",
    "    with open(\"../../datasets/prompts/guidelines.txt\", 'r', encoding='utf-8') as file_txt:\n",
    "        for line in file_txt:\n",
    "            guidelines += line\n",
    "    with open(\"../../datasets/prompts/few_shots_Llama31.txt\", 'r', encoding='utf-8') as file_txt:\n",
    "        for line in file_txt:\n",
    "            few_shots += line\n",
    "\n",
    "    try:\n",
    "        # 传入的是TPL_prompt, 里边有format函数要用的{title}和{abstract}。\n",
    "        # 传入的paper会给键值对\n",
    "        # 【改】把原来的prompt加到了abstract里边\n",
    "        paper[\"abstract\"] = paper[\"title\"] + paper[\"abstract\"]\n",
    "        prompt = prompt_template.format(**paper)\n",
    "\n",
    "        # 返回的是一个json对象，有\"software\"关键字\n",
    "        # 共用llama3.1相同的guidelines和few_shots\n",
    "        Prompt_all = f\"\"\"# You are given a title and an abstract of an academic publication. Your task is to identify and extract the names of software mentioned in the abstract. Software names are typically proper nouns and may include specific tools, platforms, or libraries used in research. Please list the software names you find in the publication in a JSON object using a key \"software\". If you are unable to identify any software names, please return an empty list of \"software\". When identifying software names, please consider the following exclusion criteria \n",
    "                            Also, apply following \\\"Guidelines\\\" and refer following \\\"Gold Examples\\\" to help with accuracy \\n\"\"\"\\\n",
    "                            f\"# Guidelines: {guidelines} \\n\"\\\n",
    "                            f\"# Gold Examples: {few_shots} \\n\"\\\n",
    "                            f\"# INPUT: {prompt} \\n\"\\\n",
    "                            f\"\\n\"\\\n",
    "                            f\"# OUTPUT: \\n\"\n",
    "        \n",
    "        client = OpenAI(api_key = OPENAI_APIKEY)\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": Prompt_all}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0,\n",
    "        )\n",
    "        result = json.loads(completion.choices[0].message.content)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'! error: {e}')\n",
    "        # print full stack\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def llama_31_infer(paper):\n",
    "    print(\"* llama 3.1 is inferring\")\n",
    "    tmp = extract(SYSTEM_ROLE, TPL_PROMPT, paper)\n",
    "    if tmp is None:\n",
    "        # no software names found or error\n",
    "        result = {'software': []}\n",
    "    else:\n",
    "        # add context to the extracted entities\n",
    "        try:\n",
    "            software_names_with_contexts = get_contexts(tmp['software'], paper)\n",
    "        except Exception as e:\n",
    "            # 可能model返回的格式不是字典 or 是字典，但是没有“software”这个键，导致上一句执行错误\n",
    "            # 【对model不按指示推理设置保险 -- 不会在没\"software\"这个键上报错】\n",
    "            print(f'! error: {e}')\n",
    "            print(f'! failed to get context for {tmp}')\n",
    "            software_names_with_contexts = []\n",
    "\n",
    "        result = {'software': software_names_with_contexts}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa5e9f-a787-4737-93bd-c2bb048ab50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/920 [00:01<17:25,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:03<28:22,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['sql-92', 'compatible', 'relational', 'database'], ['pro7', '-', 'communication', 'server'], ['osf', '/', 'dce'], ['protocol', 'definition', 'language', 'compiler'], ['pro7-communication', 'server'], ['osf/dce']]\n",
      "txt to bio ========== ['sql-92', 'compatible', 'relational', 'database']\n",
      "txt to bio ========== ['pro7-communication', 'server']\n",
      "txt to bio ========== ['osf/dce']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/920 [00:04<22:43,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/920 [00:07<30:26,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/920 [00:09<29:09,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['full', 'window', 'stereo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/920 [00:10<25:29,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/920 [00:11<22:41,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['apex']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/920 [00:12<21:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mopac'], ['modeller'], ['triton']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/920 [00:14<22:40,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/920 [00:15<20:18,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/920 [00:16<18:43,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['s-plus']]\n",
      "txt to bio ========== ['s-plus']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 12/920 [00:17<18:07,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/920 [00:18<16:57,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['polylink'], ['microsoft', 'windows']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/920 [00:19<18:21,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visual', 'basic'], ['microsoft', 'word'], ['molecular', 'biocomputing', 'suite'], ['biotechniques', 'software', 'library'], ['microsoft', 'word', 'add-in']]\n",
      "txt to bio ========== ['microsoft', 'word', 'add-in']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/920 [00:20<17:13,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visual', 'basic'], ['microsoft', 'powerpoint']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/920 [00:22<17:56,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cdart']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/920 [00:23<17:34,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bioperl'], ['genquire']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/920 [00:24<17:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cinema-mx']]\n",
      "txt to bio ========== ['cinema-mx']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/920 [00:25<17:44,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['acdsee'], ['after', 'shot'], ['fotoangelo'], ['smart', 'send']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/920 [00:27<21:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visual', 'basic', '4.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/920 [00:29<25:31,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bibi']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/920 [00:30<22:34,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bioeditor']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 23/920 [00:32<22:25,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cerr'], ['aapm/rtog', 'archiving', 'mechanism'], ['c/c++'], ['fortran'], ['matlab'], ['java']]\n",
      "txt to bio ========== ['aapm/rtog', 'archiving', 'mechanism']\n",
      "txt to bio ========== ['c/c++']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 24/920 [00:33<22:09,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['modweb'], ['modbase'], ['modeller'], ['modloop'], ['snpweb'], ['modview'], ['moulder']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/920 [00:34<20:21,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gosurfer'], ['chipinfo'], ['dchip']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26/920 [00:35<18:39,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['clustal', 'series', 'of', 'programs'], ['clustal'], ['the', 'clustal', 'series', 'of', 'programs']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/920 [00:37<19:22,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['solaris.x86'], ['gibbs', 'recursive', 'sampler'], ['linux'], ['solaris'], ['gibbs', 'motif', 'sampler']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/920 [00:38<20:43,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/920 [00:40<19:21,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['stivid'], ['vbscript']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/920 [00:41<20:22,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pc', 'emulator', 'under', 'windows'], ['the', 'hp49g', 'programmable', 'calculator'], ['microsoft', 'windows'], ['metastats']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/920 [00:44<26:53,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 32/920 [00:45<23:52,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cadlive']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 33/920 [00:46<22:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['unix', 'workstations'], ['ibis'], ['fortran', '77']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 34/920 [00:49<26:05,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['homgl']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 35/920 [00:50<25:28,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'microsoft', 'windows', '3.0', 'operating', 'system'], ['universal', 'data', 'acquisition', 'program'], ['windows', '3.0'], ['microsoft', 'windows', '3.0'], ['dynamic', 'link', 'libraries', '(dll)']]\n",
      "txt to bio ========== ['dynamic', 'link', 'libraries', '(dll)']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 36/920 [00:51<22:03,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['nonmem']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 37/920 [00:52<20:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['rounds'], ['help', 'system']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38/920 [00:55<24:56,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['chull.sas']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 39/920 [00:56<22:28,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['moltalk']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 40/920 [00:59<27:01,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ensembl', 'core', 'software', 'libraries']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/920 [01:00<25:28,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['svm-based', 'method', 'for', 'subcellular', 'localization', 'of', 'eukaryotic', 'proteins', 'using', 'dipeptide', 'composition', 'and', 'psi-blast'], ['eslpred']]\n",
      "txt to bio ========== ['svm-based', 'method', 'for', 'subcellular', 'localization', 'of', 'eukaryotic', 'proteins', 'using', 'dipeptide', 'composition', 'and', 'psi-blast']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 42/920 [01:01<23:03,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['probelynx']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 43/920 [01:02<21:02,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['genepalette']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 44/920 [01:03<19:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['haplore'], ['haplotype', 'reconstruction']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 45/920 [01:04<18:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bioconductor'], ['rmageml']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 46/920 [01:06<18:08,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['goget'], ['java', '2', 'enterprise', 'edition', 'technology'], ['gene', 'ontology', 'database'], ['goview']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 47/920 [01:07<17:41,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['muscle']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 48/920 [01:08<16:47,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['varmixt'], ['r', 'package']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 49/920 [01:09<16:44,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['clann'], ['apple', 'macintosh'], ['linux'], ['windows', 'operating', 'systems']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/920 [01:10<16:13,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 51/920 [01:11<17:06,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['paris', 'genome', 'rearrangement', 'server'], ['paris', 'genome', 'rearrangement']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 52/920 [01:13<17:14,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cgview']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 53/920 [01:14<16:27,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pcr'], ['clustal', 'x']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 54/920 [01:15<18:11,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ccp4', 'coordinate', 'library']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 55/920 [01:16<17:15,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['vms'], ['overseer'], ['unix']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 56/920 [01:17<17:03,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel', 'for', 'windows']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 57/920 [01:19<17:16,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windows'], ['linux'], ['java', 'application'], ['cghpro'], ['array', 'cgh']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 58/920 [01:20<16:34,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 59/920 [01:21<16:33,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['hdbstat!']]\n",
      "txt to bio ========== ['hdbstat!']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 60/920 [01:22<17:14,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['egs4'], ['fortran', '77'], ['bbn', 'tc2000'], ['pc/386/387']]\n",
      "txt to bio ========== ['pc/386/387']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 61/920 [01:23<17:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ucsc', 'gene', 'sorter']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 62/920 [01:25<17:54,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['python'], ['qhull'], ['provat'], ['pymol']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 63/920 [01:26<19:32,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['locsvmpsi'], ['locnet'], ['targetp'], ['eslpred'], ['psi-blast'], ['psortii'], ['svm'], ['subloc']]\n",
      "txt to bio ========== ['psi-blast']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 64/920 [01:28<19:14,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['lga'], ['psi-blast'], ['pdb'], ['al2ts'], ['as2ts']]\n",
      "txt to bio ========== ['psi-blast']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 65/920 [01:29<17:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['psi-blast'], ['modeller']]\n",
      "txt to bio ========== ['psi-blast']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 66/920 [01:30<18:19,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['department', 'of', 'informatics'], ['linux'], ['colotux'], ['siemens'], ['colonography']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 67/920 [01:32<20:13,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['medusa']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 68/920 [01:33<19:28,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'software'], ['microsoft', 'foundation', 'class', 'library', '(mfc)'], ['vc++', '6.0']]\n",
      "txt to bio ========== ['microsoft', 'foundation', 'class', 'library', '(mfc)']\n",
      "txt to bio ========== ['vc++', '6.0']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 69/920 [01:34<17:53,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 70/920 [01:35<16:32,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['r'], ['ocplus']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 71/920 [01:36<15:39,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mzmine']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 72/920 [01:37<15:20,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['turbo', 'pascal'], ['chelator']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 73/920 [01:39<17:54,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['jaf'], ['http://www.proteomecommons.org/current/511/'], ['proteomecommons.org', 'jaf'], ['java', 'analysis', 'framework', '(jaf)']]\n",
      "txt to bio ========== ['http://www.proteomecommons.org/current/511/']\n",
      "txt to bio ========== ['java', 'analysis', 'framework', '(jaf)']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 74/920 [01:40<18:44,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ciphergen', 'proteinchip', 'software', '3.1']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 75/920 [01:42<20:17,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['genechip', 'operating', 'software', '(mas', 'or', 'gcos)'], ['dchip', 'pm'], ['rma'], ['microarray', 'analysis', 'suite'], ['gc-rma'], ['pdnn'], ['dchip', 'pmmm']]\n",
      "txt to bio ========== ['genechip', 'operating', 'software', '(mas', 'or', 'gcos)']\n",
      "txt to bio ========== ['gc-rma']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 76/920 [01:43<19:08,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bioconductor'], ['orderedlist']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 77/920 [01:44<18:41,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'migenas', 'integrated', 'bioinformatics', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 78/920 [01:46<19:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['masqot'], ['java'], ['masqot-gui'], ['gnu', 'lgpl']]\n",
      "txt to bio ========== ['masqot-gui']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 79/920 [01:47<18:04,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 80/920 [01:49<19:38,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['examiner', 'module'], ['project', 'manager', 'module'], ['extractor', 'module'], ['pet-tool'], ['mapper', 'module']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 81/920 [01:50<18:24,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['m-gcat']]\n",
      "txt to bio ========== ['m-gcat']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 82/920 [01:51<18:37,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['opengl'], ['openmaf'], ['dicom']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 83/920 [01:52<17:35,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bhageerath']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 84/920 [01:53<16:26,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['azyxxi'], ['microsoft']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 85/920 [01:54<16:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visual', 'basic'], ['matlab'], ['viskin']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 86/920 [01:56<16:42,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['stampa'], ['gerbil'], ['haploview'], ['gevalt']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 87/920 [01:57<16:35,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['blm', 'analyzer'], ['the', 'blm', 'analyzer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 88/920 [01:58<16:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ms-windows'], ['wsxm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 89/920 [01:59<16:54,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['casmil']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 90/920 [02:00<16:30,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gepat'], ['genome', 'expression', 'pathway', 'analysis', 'tool']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 91/920 [02:02<18:09,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['svm-fold']]\n",
      "txt to bio ========== ['svm-fold']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 92/920 [02:05<24:35,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['tassel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 93/920 [02:06<21:27,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gromacs'], ['guimacs']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 94/920 [02:07<20:49,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['blast'], ['nwaycomp'], ['phylip'], ['clustalw'], ['primer3'], ['align']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 95/920 [02:08<18:55,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel', '2004'], ['visual', 'basic', 'for', 'applications']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 96/920 [02:10<18:40,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['daisy']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 97/920 [02:12<23:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pycogent']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 98/920 [02:13<22:07,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visualization', 'toolkit'], ['windows'], ['linux'], ['unix'], ['cavass'], ['insight', 'toolkit'], ['mac']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 99/920 [02:15<22:19,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['i-adhore', '2.0'], ['unix'], ['linux'], ['i-adhore']]\n",
      "txt to bio ========== ['i-adhore', '2.0']\n",
      "txt to bio ========== ['i-adhore']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 100/920 [02:17<21:31,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windows-based', 'software', 'tool'], ['ampe'], ['iso', '5725'], ['fuzzy', 'logic'], ['analytical', 'method', 'performance', 'evaluation', '(ampe)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 101/920 [02:18<19:18,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'visual', 'c/c++'], ['act', '4']]\n",
      "txt to bio ========== ['microsoft', 'visual', 'c/c++']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 102/920 [02:20<24:28,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 103/920 [02:21<21:40,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['swan']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 104/920 [02:23<19:57,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fmrib', 'software', 'library'], ['bet']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 105/920 [02:24<18:06,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'word'], ['endnote'], ['pubmed']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 106/920 [02:25<18:11,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['r', 'package', 'hcgene'], ['hcgene'], ['funcat', 'taxonomy'], ['gene', 'ontology']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 107/920 [02:26<17:23,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['datarail']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 108/920 [02:28<18:02,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['graphcrunch']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 109/920 [02:29<18:25,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['perl'], ['c'], ['c++'], ['java'], ['python'], ['c#']]\n",
      "txt to bio ========== ['c++']\n",
      "txt to bio ========== ['c#']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 110/920 [02:30<17:30,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['root'], ['ctmod'], [\"cern's\", 'application', 'development', 'framework']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 111/920 [02:31<16:31,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['dovis'], ['linux', 'cluster'], ['autodock']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 112/920 [02:34<21:20,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['census']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 113/920 [02:35<19:16,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['perl'], ['bioperl'], ['java', 'applet']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 114/920 [02:36<18:21,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['sculpter']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 115/920 [02:37<16:58,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ushuffle']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 116/920 [02:38<17:08,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['varivis'], ['database', 'management', 'systems'], ['perl', 'cgi', 'scripts'], ['varivis', 'software', 'package']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 117/920 [02:40<17:50,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['patman']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 118/920 [02:41<16:36,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['toxmatch']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 119/920 [02:44<23:03,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'visual', 'basic', '6.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 120/920 [02:45<21:17,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['skdm'], ['java', 'application']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 121/920 [02:46<19:43,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['potterswheel'], ['matlab', 'with', 'optimization', 'toolbox'], ['matlab', 'toolbox']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 122/920 [02:47<19:03,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['procope'], ['java', 'software', 'suite']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 123/920 [02:49<21:21,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gibbsmarkov', 'with', 'significance', 'analysis'], ['gimsan']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 124/920 [02:51<20:11,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['domaingraph']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 125/920 [02:52<19:33,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['unafold'], ['mac', 'os', 'x'], ['linux'], ['unix']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 126/920 [02:55<23:14,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 127/920 [02:56<20:43,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fluxplus'], ['microsoft', 'gwbasic'], ['ibm-pc']]\n",
      "txt to bio ========== ['ibm-pc']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 128/920 [02:57<19:46,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 129/920 [02:58<18:59,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mazda']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 130/920 [03:00<18:47,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fax06'], ['max06'], ['caldose_x']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 131/920 [03:01<17:06,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 132/920 [03:02<16:16,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['java', 'starlogo', '2.0'], ['acacia']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 133/920 [03:03<15:58,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pazar', 'database'], ['orca', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 134/920 [03:04<15:22,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['alc']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 135/920 [03:05<15:55,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pconpy']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 136/920 [03:07<16:16,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windows', 'script', 'host', '(wsh)'], ['activex', 'data', 'objects'], ['javascript'], ['ole', 'automation'], ['sql']]\n",
      "txt to bio ========== ['windows', 'script', 'host', '(wsh)']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 137/920 [03:08<15:53,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ez-rhizo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 138/920 [03:09<15:54,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'word']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 139/920 [03:10<16:37,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mcnp(x)', 'monte', 'carlo', 'computer', 'code'], ['sesame--simulation', 'of', 'external', 'source', 'accident', 'with', 'medical', 'images'], ['sesame']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 140/920 [03:12<17:25,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gene', 'expression', 'omnibus', 'database'], ['tbrowser'], ['david', 'knowledgebase'], ['transcriptomebrowser'], ['java', 'application'], ['markov', 'clustering', 'algorithm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 141/920 [03:13<17:36,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['decyder', 'v6.5'], ['dymension', '3'], ['progenesis', 'samespots', 'v3.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 142/920 [03:14<16:32,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['prep', '+', '07']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 143/920 [03:16<15:55,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ajax'], ['google', 'maps', 'api'], ['genome', 'projector']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 144/920 [03:17<15:13,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['associationviewer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 145/920 [03:18<15:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['genome', 'reverse', 'compiler']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 146/920 [03:19<15:53,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['va-batts'], ['qct'], ['fe']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 147/920 [03:22<20:48,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm'], ['bisen'], ['biochemical', 'simulation', 'environment']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 148/920 [03:23<20:12,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['jaspar'], ['transfac'], ['cotrasif']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 149/920 [03:24<18:17,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mixture', 'subroutine', 'in', 'nonmem'], ['nonmem']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 150/920 [03:25<17:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm'], ['microsoft', 'fortran', 'version', '5.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 151/920 [03:26<16:20,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ingeneue']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 152/920 [03:29<22:27,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ensembl', 'software'], ['the', 'microbe', 'browser']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 153/920 [03:30<19:50,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['3d-dart'], ['3dna'], ['python']]\n",
      "txt to bio ========== ['3d-dart']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 154/920 [03:32<18:31,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['open', 'source', 'tools'], ['the', 'geant4', 'application', 'for', 'tomographic', 'emission', 'simulation', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 155/920 [03:33<18:28,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['modular', 'preoperative', 'planning', 'software'], ['insight', 'toolkit', '(kitware,', 'inc.)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 156/920 [03:35<19:35,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['rtracklayer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 157/920 [03:36<18:06,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['statoolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 158/920 [03:37<16:34,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['entrez'], ['pubmed'], ['pipeline', 'pilot']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 159/920 [03:38<15:29,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['fmri'], ['pet']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 160/920 [03:39<15:23,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['esi-ms'], ['maldi-ms'], ['glycoworkbench']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 161/920 [03:42<21:17,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 162/920 [03:43<18:44,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 163/920 [03:44<16:42,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 164/920 [03:45<16:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gnu', 'general', 'public', 'license'], ['mspecs']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 165/920 [03:47<16:43,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['r/bioconductor'], ['crlmm'], [\"illumina's\", 'infinium', 'whole-genome', 'genotyping', 'beadchips']]\n",
      "txt to bio ========== ['r/bioconductor']\n",
      "txt to bio ========== [\"illumina's\", 'infinium', 'whole-genome', 'genotyping', 'beadchips']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 166/920 [03:47<15:07,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['java', 'tool', 'kit', 'for', 'building', 'genomics', 'visualization', 'applications'], ['genoviz', 'software', 'development', 'kit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 167/920 [03:49<14:59,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['saint'], ['libsbml'], ['google', 'web', 'toolkit'], ['tomcat']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 168/920 [03:50<16:56,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 169/920 [03:52<16:20,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ptmsearchplus']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 170/920 [03:53<16:37,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['limsa'], ['secd']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 171/920 [03:54<17:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['easymifs'], ['easymifs'], ['sitehound']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 172/920 [03:56<18:30,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['natbox'], ['r', 'statistical', 'language']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 173/920 [03:58<17:51,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['lipid'], ['visual', 'basic', 'for', 'applications'], ['ms', 'excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 174/920 [03:59<16:45,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['survival', 'online']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 175/920 [04:00<16:19,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['matlab', 'component', 'runtime'], ['gene', 'armada']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 176/920 [04:01<15:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 177/920 [04:02<15:39,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cone-beam', 'ct'], ['simplant'], ['materialise', 'dental'], ['surgiguide'], ['ct', 'images']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 178/920 [04:03<14:38,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mapnext']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 179/920 [04:05<14:53,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['seqbuster']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 180/920 [04:06<15:36,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['poptree2'], ['windows', 'interface']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 181/920 [04:07<15:09,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['svmprat']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 182/920 [04:08<14:26,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm']]\n",
      "txt to bio ========== ['therm']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 183/920 [04:09<13:56,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 184/920 [04:10<13:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['nemo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 185/920 [04:12<15:16,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windows,', 'macintosh,', 'or', 'linux', 'operating', 'system'], ['pc', 'windows', 'operating', 'systems'], ['jcms']]\n",
      "txt to bio ========== ['windows,', 'macintosh,', 'or', 'linux', 'operating', 'system']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 186/920 [04:13<15:50,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['consensuscluster']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 187/920 [04:14<15:41,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gosemsim'], ['bioconductor', 'project'], ['r', 'package']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 188/920 [04:15<14:43,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 189/920 [04:16<13:55,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['solid'], ['perl']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 190/920 [04:18<14:20,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['rnastructure']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 191/920 [04:19<14:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['diacontrol']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 192/920 [04:20<14:49,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['armone']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 193/920 [04:22<18:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['distmatcomp']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 194/920 [04:23<16:44,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm'], ['qspect']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 195/920 [04:25<15:53,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fall', 'tips', 'toolkit'], ['fall', 'tips']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 196/920 [04:26<14:53,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 197/920 [04:27<14:46,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cochlear', 'implant', 'processor', 'programming'], ['fitting', 'to', 'outcomes', 'expert']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 198/920 [04:28<13:59,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 199/920 [04:29<14:50,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['emr'], ['road'], ['sarma']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 200/920 [04:30<13:33,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['learning', 'assessment', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 201/920 [04:32<15:58,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['kaks_calculator', '2.0'], ['gamma-series', 'methods'], ['slidingwindow', 'strategies']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 202/920 [04:33<15:36,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['goal'], ['gene', 'ontology', 'analyzer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 203/920 [04:35<16:33,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['homstrad'], ['tcoffee'], ['mafft'], ['mtrap'], ['clustalw2'], ['prefab', '4.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 204/920 [04:36<15:34,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['guidance']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 205/920 [04:37<15:44,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['compasss', '(complex', 'pattern', 'of', 'sequence', 'search', 'software)'], ['compasss', 'suite'], ['compasss']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 206/920 [04:38<15:07,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['jcoda']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 207/920 [04:40<14:49,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['modevo'], ['networkevolution'], ['cytoscape'], ['apcluster']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 208/920 [04:41<14:03,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 209/920 [04:42<14:12,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['merger'], ['profiler'], ['viewer'], ['profiler-merger-viewer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 210/920 [04:43<13:36,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['kbsim']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 211/920 [04:44<13:14,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 212/920 [04:45<13:38,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bionetgen', 'language', '(bngl)'], ['dynstoc'], ['rulemonkey']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 213/920 [04:46<13:34,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mrmap']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 214/920 [04:48<14:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bioruby']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 215/920 [04:49<13:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['decision', 'peptide-driven'], ['dpd', 'software']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 216/920 [04:50<13:06,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['v-xtractor']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 217/920 [04:51<13:07,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metagenomethreader']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 218/920 [04:52<13:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metabolic', 'design']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 219/920 [04:55<17:48,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['chembench']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 220/920 [04:56<15:54,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cdms']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 221/920 [04:57<16:30,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['biodosimetry', 'assessment', 'tool', '(bat)'], ['microsoft', 'visual', 'basic', '6'], ['armed', 'forces', 'radiobiology', 'research', \"institute's\", 'biological', 'dosimetry', 'research', 'program']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 222/920 [04:59<17:17,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'aqcel', 'method'], ['the', 'application', 'software', 'aqcel'], ['the', 'conventional', 'method'], ['aqcel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 223/920 [05:00<15:51,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['omicsanalyzer'], ['cytoscape'], ['java']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 224/920 [05:01<14:49,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['google', 'maps'], ['celldesigner'], ['cellpublisher']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 225/920 [05:02<14:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mist'], ['mitre', 'identification', 'scrubber', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 226/920 [05:03<13:55,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['paragon'], ['excel'], ['compid'], ['mascot']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 227/920 [05:04<12:40,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fall', 'tips']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 228/920 [05:06<15:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 229/920 [05:08<17:07,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['consensx']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 230/920 [05:09<16:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cagrid', 'workflow', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 231/920 [05:12<20:45,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 232/920 [05:14<22:57,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 233/920 [05:15<20:01,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['itk', 'software', 'framework'], ['julide']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 234/920 [05:17<18:31,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['netpath-win'], ['netpath'], ['microsoft', 'windows']]\n",
      "txt to bio ========== ['netpath-win']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 235/920 [05:18<16:30,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 236/920 [05:19<15:35,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['onto-toolkit'], ['onto-perl'], ['galaxy']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 237/920 [05:20<14:48,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['arrayqualitymetrics']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 238/920 [05:21<14:10,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['caes'], ['chinese', 'acupuncture', 'expert', 'system']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 239/920 [05:22<14:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['nuclearquant'], ['leica', 'bond', 'max', 'system']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 240/920 [05:23<13:48,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gepoclu']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 241/920 [05:25<14:21,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['dicom'], ['qt', 'graphical', 'user', 'interface'], ['nirviz'], ['visualization', 'toolkit', 'library']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 242/920 [05:26<13:14,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['blast']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 243/920 [05:28<17:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['geant4', 'simulation', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 244/920 [05:29<16:30,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['dchip', 'survival', 'analysis', 'module'], ['dchip', 'software'], ['visual', 'c++'], ['dchip']]\n",
      "txt to bio ========== ['visual', 'c++']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 245/920 [05:31<15:04,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 246/920 [05:32<17:05,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mzml'], ['tool-chain'], ['mzmatch'], ['r', 'library'], ['peakml', 'viewer'], ['mzdata'], ['peakml'], ['java', 'library'], ['mzxml']]\n",
      "txt to bio ========== ['tool-chain']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 247/920 [05:34<16:07,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['anyexpress']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 248/920 [05:35<14:54,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['geneious'], ['species', 'delimitation']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 249/920 [05:36<14:12,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['gc×gc/tofms'], ['guineu']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 250/920 [05:37<13:55,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bc-genexminer'], ['mysql', 'relational', 'database'], ['r', 'statistical', 'software']]\n",
      "txt to bio ========== ['bc-genexminer']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 251/920 [05:38<13:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['splash'], ['caret'], ['splash']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 252/920 [05:39<12:41,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 253/920 [05:40<13:14,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mirpara'], ['mirbase'], ['hts'], ['svm'], ['mirpara']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 254/920 [05:42<12:58,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['binoch']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 255/920 [05:43<13:25,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['sim4cc'], ['leaff'], ['leaff'], ['sim4db'], ['sim4db']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 256/920 [05:44<13:14,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windows'], ['spyder'], ['linux'], ['mac']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 257/920 [05:45<12:55,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['nsmap']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 258/920 [05:46<12:52,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['hmmer3'], ['pfam'], ['hmmer']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 259/920 [05:47<12:46,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['clotho']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 260/920 [05:49<13:25,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['wxpython'], ['relaxgui'], ['python'], ['nmr']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 261/920 [05:50<14:20,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['labkey', \"server's\", 'nab', 'tool'], ['atlas', 'science', 'portal'], ['labkey', 'server'], ['excel', 'macro']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 262/920 [05:51<13:46,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ontocat']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 263/920 [05:53<13:33,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mm-usc*pack']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 264/920 [05:54<13:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metaxa']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 265/920 [05:55<13:58,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['version', '01.10'], ['version', '3.02'], ['flotrac/vigileo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 266/920 [05:57<14:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['snvbox'], ['linux', 'system'], ['mysql', 'database'], ['c++'], ['python'], ['chasm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 267/920 [05:58<14:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'proteored', 'miape', 'web', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 268/920 [05:59<13:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['orbitview']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 269/920 [06:00<13:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['wommbat'], ['linux'], ['windows'], ['mac']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 270/920 [06:02<14:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['msoar'], ['notung'], ['multiparanoid'], ['multimsoar'], ['multimsoar', '2.0']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 271/920 [06:03<15:53,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['human', 'proteins'], ['fold', 'and', 'function', 'assignment', 'system'], ['microbial', 'virulence', 'factors'], ['nucleic', 'acids', 'research'], ['metagenomic', 'sequences'], ['ffas03'], ['ffas'], ['protein', 'science'], ['complete', 'proteomes']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 272/920 [06:05<14:59,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['geoviz', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 273/920 [06:06<13:51,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 274/920 [06:07<12:43,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['galaxy'], ['python']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 275/920 [06:08<12:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['shap'], ['java']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 276/920 [06:10<14:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ecancercare(bladder)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 277/920 [06:11<13:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 278/920 [06:12<12:40,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['zoneminder'], ['zm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 279/920 [06:13<12:35,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['enrichment', 'map'], ['cytoscape']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 280/920 [06:14<12:39,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'chembl', 'database'], ['cytoscape']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 281/920 [06:15<12:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['cobra', 'toolbox']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 282/920 [06:17<13:17,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['syngo.via', 'ct', 'vascular'], ['syngo.via', '(siemens', 'healthcare,', 'forchheim,', 'germany)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 283/920 [06:18<13:19,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['afni'], ['rest'], ['spm']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 284/920 [06:19<14:36,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['drupal'], ['interpro'], ['gbrowse'], ['gene', 'ontology'], ['ncbi', 'blast'], ['tripal'], ['kyoto', 'encyclopedia', 'of', 'genes', 'and', 'genomes'], ['chado']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 285/920 [06:21<13:50,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['javascript'], ['krona'], ['web', 'browser'], ['html5']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 286/920 [06:22<12:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['prots']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 287/920 [06:23<12:09,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['neuritequant'], ['imagej']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 288/920 [06:24<12:39,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['eyemap']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 289/920 [06:25<12:27,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['sspace'], ['sopra'], ['mip', 'scaffolder']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 290/920 [06:27<13:26,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['linux'], ['bdtcomparator'], ['windows', 'operating', 'systems']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 291/920 [06:28<12:46,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['rest-gca'], ['matlab'], ['rest']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 292/920 [06:29<12:03,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ntrfinder']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 293/920 [06:30<12:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['jamie']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 294/920 [06:31<11:39,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['logviewer'], ['rawxtract']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 295/920 [06:32<12:54,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metscape'], ['cytoscape', 'plugin', 'manager'], ['ehmn', 'databases'], ['kegg'], ['metscape', '2', 'bioinformatics', 'tool']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 296/920 [06:35<15:52,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['monte', 'carlo', '(mc)', 'simulation'], ['maximum', 'likelihood-expectation', 'maximization', '(mlem)'], ['geant4'], ['integral', 'of', 'the', 'signal', 'in', 'each', 'mass', 'lesion', '(integrated', 'mass', 'signal,', 'ims)'], ['signal-difference-to-noise', 'ratio', '(sdnr)'], ['modulation', 'transfer', 'function', '(mtf)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 297/920 [06:36<15:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['windose'], ['impact', 'ct', 'patients', 'dosimetry', 'calculator'], ['ct-expo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 298/920 [06:37<13:42,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 299/920 [06:38<13:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ptools'], ['c++'], ['python']]\n",
      "txt to bio ========== ['c++']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 300/920 [06:39<12:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['convan']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 301/920 [06:40<12:29,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mcscan'], ['mcscanx']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 302/920 [06:41<12:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['mapcheck', '2'], ['3dvh', 'software']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 303/920 [06:42<11:50,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metextract']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 304/920 [06:44<11:58,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pyelph'], ['python']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 305/920 [06:45<12:43,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['the', 'ieee', '11073', 'personal', 'health', 'device', '(phd)', 'group'], ['iso/ieee', '11073', 'phd', 'message', 'generation', 'toolkit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 306/920 [06:46<12:41,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['homeml', 'repository'], ['homeml', 'toolkit'], ['homeml', 'application']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 307/920 [06:48<14:26,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['meme/mast'], ['meme'], ['r', 'package'], ['iteme'], ['mdscan'], ['meet'], ['clustalw'], ['match'], ['q-residuals'], ['muscle']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 308/920 [06:49<13:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['kurzweil', '3000']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 309/920 [06:51<13:21,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['copicat']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 310/920 [06:52<13:05,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['anntools']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 311/920 [06:53<13:36,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ngs', 'qc', 'toolkit'], ['roche', '454'], ['illumina'], ['perl']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 312/920 [06:54<11:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 313/920 [06:55<12:02,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ecomics']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 314/920 [06:56<12:01,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['biogem'], ['windows'], ['ruby'], ['linux'], ['mac', 'os', 'x']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 315/920 [06:58<12:05,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['flotrac/vigileo', 'system'], ['third', 'generation', 'software', 'version', '3.02']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 316/920 [06:59<11:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['matlab'], ['sos']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 317/920 [07:00<11:59,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['microsoft', 'excel'], ['freedom', 'evo', 'liquid', 'handler', 'software']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 318/920 [07:01<11:30,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ceawatch']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 319/920 [07:02<11:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['cellulose-builder'], ['the', 'bash', 'programming', 'language']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 320/920 [07:03<11:15,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ms-excel']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 321/920 [07:04<10:53,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fcstrans']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 322/920 [07:05<10:46,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['icd-10', 'toolkit'], ['icd-10']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 323/920 [07:07<11:13,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['tina', 'manual', 'landmarking', 'tool']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 324/920 [07:08<11:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['visual', 'exploration', 'and', 'statistics', 'to', 'promote', 'annotation'], ['vespa']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 325/920 [07:09<11:45,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['rosetta'], ['saber'], ['rosettadesign']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 326/920 [07:10<11:48,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['face', 'software'], ['face']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 327/920 [07:11<11:19,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm']]\n",
      "txt to bio ========== ['therm']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 328/920 [07:13<12:20,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['isoquant'], ['windows', '7']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 329/920 [07:14<12:03,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['psi-search']]\n",
      "txt to bio ========== ['psi-search']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 330/920 [07:17<18:03,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['bayesian', 'regularization', 'method'], ['prior'], ['one-way', 'analysis', 'of', 'variance'], ['dna', 'microarrays'], ['background'], ['multiple', 'tests', 'correction'], ['next-generation', 'sequencing', '(rna-seq)'], ['empirical', 'measurements'], ['diagnostic', 'plots'], ['quantitative', 'mass', 'spectrometry'], ['protein', 'arrays'], ['data', 'sets'], ['two-sample', 't-tests'], ['regularized', 'variance'], ['t-test'], ['r', 'source', 'code'], ['probabilistic', 'mixture', 'model', 'treatment'], ['cyber-t']]\n",
      "txt to bio ========== ['one-way', 'analysis', 'of', 'variance']\n",
      "txt to bio ========== ['next-generation', 'sequencing', '(rna-seq)']\n",
      "txt to bio ========== ['two-sample', 't-tests']\n",
      "txt to bio ========== ['t-test']\n",
      "txt to bio ========== ['cyber-t']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 331/920 [07:19<16:39,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['firsst4'], ['asa24'], ['firsst4']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 332/920 [07:20<14:42,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['tps'], ['eclipse'], ['diamond']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 333/920 [07:21<13:28,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fice']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 334/920 [07:22<13:52,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['assign-sbt', 'v3.6+'], ['assign-sbt', 'v3.2.7'], ['conexio', 'genomics']]\n",
      "txt to bio ========== ['assign-sbt', 'v3.6+']\n",
      "txt to bio ========== ['assign-sbt', 'v3.2.7']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 335/920 [07:23<13:09,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['nrpb-sr250', 'software'], ['nrpb-sr250']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 336/920 [07:24<12:18,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['stepstone', 'interactive', 'medical', 'software']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 337/920 [07:26<11:48,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['pagit']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 338/920 [07:27<11:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 339/920 [07:28<11:18,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['amber'], ['dendrimer', 'building', 'toolkit'], ['dbt']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 340/920 [07:30<14:55,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['simupop', 'simulation', 'environment'], ['gene-environment', 'interaction', 'simulator', '2', '(gens2)'], ['python', 'language']]\n",
      "txt to bio ========== ['gene-environment', 'interaction', 'simulator', '2', '(gens2)']\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 341/920 [07:31<13:31,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['ephla', 'software'], ['ephla', 'method'], ['ephla', 'program'], ['ephla']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 342/920 [07:32<12:31,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['minfi', 'bioconductor', 'package'], ['swan']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 343/920 [07:34<12:08,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['therm'], ['primer-blast']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 344/920 [07:35<12:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 345/920 [07:36<11:42,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['kmwin'], ['sas'], ['spss'], ['r']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 346/920 [07:37<11:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 347/920 [07:39<12:29,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['fusionfinder'], ['rna-seq', 'read', 'data'], ['perl-based', 'software'], ['single-end', '(se)', 'or', 'paired-end', '(pe)']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 348/920 [07:40<11:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['metabosearch']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 349/920 [07:41<11:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['milquant']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 350/920 [07:42<12:22,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### [['lxtoo']]\n",
      "* llama 3.1 is inferring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 351/920 [07:44<13:15,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### []\n",
      "* llama 3.1 is inferring\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# 训练+测试集数据\n",
    "train_folder_path = \"../../datasets/train_data\"\n",
    "test_folder_path = \"../../datasets/test_gold\"\n",
    "train_files = sorted(os.listdir(train_folder_path))\n",
    "test_files = sorted(os.listdir(test_folder_path))\n",
    "\n",
    "\n",
    "# 所有的train+test data\n",
    "train_path=\"../../datasets/train_data/\"\n",
    "test_path=\"../../datasets/test_gold/\"\n",
    "train_txt_files = sorted([train_path + f for f in train_files if f.endswith('.txt')])\n",
    "test_txt_files = sorted([test_path + f for f in test_files if f.endswith('.txt')])\n",
    "train_ann_files = sorted([train_path + f for f in train_files if f.endswith('.ann')])\n",
    "test_ann_files = sorted([test_path + f for f in test_files if f.endswith('.ann')])\n",
    "\n",
    "new_txt = train_txt_files + test_txt_files\n",
    "new_ann = train_ann_files + test_ann_files\n",
    "\n",
    "# 【目前按照llama3.1的格式来写，gpt-4o需要修改】\n",
    "all_gold = list()\n",
    "llama_all_pred = list()\n",
    "gpt4o_all_pred = list()\n",
    "\n",
    "merged_list = zip(new_txt, new_ann)\n",
    "gold_cant_match = 0\n",
    "\n",
    "\n",
    "for txt, ann in tqdm(merged_list, total=len(new_ann)):\n",
    "    current_paper = dict() # 包含pmid, title 和 abstract的字典\n",
    "\n",
    "    file_name = os.path.basename(txt)  \n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # 去除扩展名\n",
    "    # 此时abstract和title先分开放，到时候调llama3.1推理的时候再合并\n",
    "\n",
    "\n",
    "    # ------ txt文件 ------ #\n",
    "    txt_with_index_content = []\n",
    "    with open(txt, \"r\", encoding=\"utf-8\") as x_data: # Use writelines to write list\n",
    "        txt_content = []\n",
    "        count = -1\n",
    "        current_paper[\"pmid\"] = file_name_without_extension\n",
    "        for line in x_data:\n",
    "            # -- 让model infer的 -- #\n",
    "            count += 1\n",
    "            processed_line = line.strip()\n",
    "            if count==0:\n",
    "                current_paper[\"title\"] = processed_line\n",
    "                current_paper[\"abstract\"] = \"\"\n",
    "            else:\n",
    "                current_paper[\"abstract\"] += processed_line\n",
    "            \n",
    "            # -- 让ann生成gold bio list的 -- #\n",
    "            content = line.strip().split(\" \")\n",
    "            txt_content.extend(content)\n",
    "            cur_index = 0\n",
    "            for item in txt_content:\n",
    "                txt_with_index_content.append((cur_index, item))\n",
    "                cur_index = cur_index + len(item) + 1\n",
    "\n",
    "\n",
    "    # ------ ann文件 ------ #\n",
    "    current_gold_list = list()\n",
    "    current_gold_dict = dict()\n",
    "    with open(ann, \"r\", encoding=\"utf-8\") as y_data: # Use writelines to write list\n",
    "        for line in y_data:\n",
    "            current_mes = line.strip().split(\"\\t\")\n",
    "            \n",
    "            # 可能读到ann是空的 \n",
    "            if len(current_mes)<=3:\n",
    "                continue\n",
    "            \n",
    "            # ann非空 \n",
    "            index_initial = current_mes[1].strip().split(\" \")\n",
    "            if len(index_initial) == 3:\n",
    "                index = (int(index_initial[1]), int(index_initial[2]))\n",
    "            elif len(index_initial) == 4: \n",
    "                item_index = index_initial[2].find(\";\")\n",
    "                index = (int(index_initial[1]), int(index_initial[2][0:item_index]))\n",
    "            current_gold_list.append((index, current_mes[-1]))\n",
    "\n",
    "            current_gold_dict[index[0]] = (index[1], current_mes[-1])\n",
    "            # index[0]是entity刚开始的坐标，index[1]是entity结束的坐标, current_mes是mention的名字\n",
    "    \n",
    "\n",
    "    # ------ 生成ann文件的gold label ------ #\n",
    "    gold_current_label = list()\n",
    "    flag = False\n",
    "    last_index = 0\n",
    "    for item in txt_with_index_content:\n",
    "        men_index = item[0]\n",
    "        men_name = item[1]\n",
    "        if (flag == False) & (men_index in current_gold_dict.keys()): # matched! & B\n",
    "            gold_current_label.append(\"B\")\n",
    "            flag = True\n",
    "            last_index = current_gold_dict[men_index][0]\n",
    "        elif flag == True: # matched! & I\n",
    "            if men_index <= last_index:\n",
    "                gold_current_label.append(\"I\")\n",
    "            else: # end match & O\n",
    "                gold_current_label.append(\"O\")\n",
    "                flag = False\n",
    "        else:\n",
    "            gold_current_label.append(\"O\")\n",
    "            \n",
    "    # -- 两模型infer -- #\n",
    "    pred_llama_entities = llama_31_infer(current_paper) # 在函数内把current_paper给改了，把title加到了abstract上\n",
    "    # pred_gpt4o_entities = gpt_4o_infer(current_paper) # 在函数内把current_paper给改了，把title加到了abstract上\n",
    "    \n",
    "    # -- pred：两模型生成bio list -- #\n",
    "    pred_llama_bio_list = convert_txt_to_bio(current_paper[\"abstract\"], pred_llama_entities[\"software\"])\n",
    "    # pred_gpt4o_bio_list = convert_txt_to_bio(current_paper[\"abstract\"], pred_gpt4o_entities[\"software\"])\n",
    "    llama_all_pred += pred_llama_bio_list\n",
    "    # gpt4o_all_pred += pred_gpt4o_bio_list\n",
    "    \n",
    "    #  -- gold：ann的bio list -- #\n",
    "    all_gold += gold_current_label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
